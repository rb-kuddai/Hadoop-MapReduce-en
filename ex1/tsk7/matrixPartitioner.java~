import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.Partitioner;


public class MatrixPartitioner implements Partitioner<Text, LongWritable> {

	public void configure(JobConf job) {}
	//given in the task 6
	public static final int TOTAL_COLUMNS = 3000;

	public int getPartition(Text key, LongWritable value, int numReduceTasks) {
		int columns_per_reducer = TOTAL_COLUMNS / numReduceTasks;
		String sKey = key.toString();
		String[] splits=sKey.split("\t"); //Split the key on tab
		int column = Integer.parseInt(splits[0]); //column key
		//min to ensure that we stay within the boundaries
		//although the last reducer will receive slighltly more data points
		return Math.min(column / columns_per_reducer, numReducerTasks - 1);
	}
}
